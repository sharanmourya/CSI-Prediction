{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f994fd2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9938, 12, 512)\n",
      "(9938, 50, 512)\n",
      "6956 1988 994\n",
      "CNN2D(\n",
      "  (conv_layer): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n",
      "LSTMNetwork(\n",
      "  (lstm): LSTM(40800, 128, num_layers=2, batch_first=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=25600, bias=True)\n",
      ")\n",
      "Epoch 1/100, Train Loss: 53.55538343684056\n",
      "Epoch 1/100, Validation Loss: 2460.52632843502\n",
      "Epoch 2/100, Train Loss: 52.80952383118725\n",
      "Epoch 2/100, Validation Loss: 2412.0263168092756\n",
      "Epoch 3/100, Train Loss: 52.32511863746046\n",
      "Epoch 3/100, Validation Loss: 2362.2764756944443\n",
      "Epoch 4/100, Train Loss: 51.652886216753245\n",
      "Epoch 4/100, Validation Loss: 2291.383552672371\n",
      "Epoch 5/100, Train Loss: 50.74738681044379\n",
      "Epoch 5/100, Validation Loss: 2200.6128937251983\n",
      "Epoch 6/100, Train Loss: 49.63747063220459\n",
      "Epoch 6/100, Validation Loss: 2110.5983305431546\n",
      "Epoch 7/100, Train Loss: 48.698566569472796\n",
      "Epoch 7/100, Validation Loss: 2036.6848764570932\n",
      "Epoch 8/100, Train Loss: 47.85088201046756\n",
      "Epoch 8/100, Validation Loss: 1967.6490614149307\n",
      "Epoch 9/100, Train Loss: 47.04050745740984\n",
      "Epoch 9/100, Validation Loss: 1901.759773375496\n",
      "Epoch 10/100, Train Loss: 46.27300916443959\n",
      "Epoch 10/100, Validation Loss: 1839.4767291356648\n",
      "Epoch 11/100, Train Loss: 45.535225180443874\n",
      "Epoch 11/100, Validation Loss: 1793.136218843006\n",
      "Epoch 12/100, Train Loss: 44.82015270360451\n",
      "Epoch 12/100, Validation Loss: 1724.2696475074404\n",
      "Epoch 13/100, Train Loss: 44.03915396507494\n",
      "Epoch 13/100, Validation Loss: 1670.4190228174602\n",
      "Epoch 14/100, Train Loss: 43.3611696553443\n",
      "Epoch 14/100, Validation Loss: 1616.8746376643105\n",
      "Epoch 15/100, Train Loss: 42.72493653421595\n",
      "Epoch 15/100, Validation Loss: 1570.9628247457838\n",
      "Epoch 16/100, Train Loss: 42.09149081508019\n",
      "Epoch 16/100, Validation Loss: 1529.658947172619\n",
      "Epoch 17/100, Train Loss: 41.47462520300388\n",
      "Epoch 17/100, Validation Loss: 1482.1989842974951\n",
      "Epoch 18/100, Train Loss: 40.83439218109742\n",
      "Epoch 18/100, Validation Loss: 1437.9140489366318\n",
      "Epoch 19/100, Train Loss: 40.25537464416274\n",
      "Epoch 19/100, Validation Loss: 1404.635757688492\n",
      "Epoch 20/100, Train Loss: 39.77665891938735\n",
      "Epoch 20/100, Validation Loss: 1367.6641341920883\n",
      "Epoch 21/100, Train Loss: 39.28428943345864\n",
      "Epoch 21/100, Validation Loss: 1330.6930706690227\n",
      "Epoch 22/100, Train Loss: 38.73995443177824\n",
      "Epoch 22/100, Validation Loss: 1302.6924738808284\n",
      "Epoch 23/100, Train Loss: 38.382614319213516\n",
      "Epoch 23/100, Validation Loss: 1277.4433148096477\n",
      "Epoch 24/100, Train Loss: 38.05248982650556\n",
      "Epoch 24/100, Validation Loss: 1259.9135005890378\n",
      "Epoch 25/100, Train Loss: 37.74755923425798\n",
      "Epoch 25/100, Validation Loss: 1242.3643217540923\n",
      "Epoch 26/100, Train Loss: 37.24799529762807\n",
      "Epoch 26/100, Validation Loss: 1203.666775173611\n",
      "Epoch 27/100, Train Loss: 36.856397410622925\n",
      "Epoch 27/100, Validation Loss: 1190.5154234871031\n",
      "Epoch 28/100, Train Loss: 36.603777643466834\n",
      "Epoch 28/100, Validation Loss: 1189.3052590990824\n",
      "Epoch 29/100, Train Loss: 36.5299766934679\n",
      "Epoch 29/100, Validation Loss: 1150.9428536551338\n",
      "Epoch 30/100, Train Loss: 35.92194577012954\n",
      "Epoch 30/100, Validation Loss: 1114.1083267454117\n",
      "Epoch 31/100, Train Loss: 35.622069013075176\n",
      "Epoch 31/100, Validation Loss: 1116.7852279420883\n",
      "Epoch 32/100, Train Loss: 35.5103784343893\n",
      "Epoch 32/100, Validation Loss: 1099.6052071707588\n",
      "Epoch 33/100, Train Loss: 35.240328653725676\n",
      "Epoch 33/100, Validation Loss: 1083.8084949311756\n",
      "Epoch 34/100, Train Loss: 34.849288626157744\n",
      "Epoch 34/100, Validation Loss: 1061.1339576357886\n",
      "Epoch 35/100, Train Loss: 34.48089672873221\n",
      "Epoch 35/100, Validation Loss: 1039.1621306888642\n",
      "Epoch 36/100, Train Loss: 34.148102190147995\n",
      "Epoch 36/100, Validation Loss: 1013.7530808221726\n",
      "Epoch 37/100, Train Loss: 33.755134717139775\n",
      "Epoch 37/100, Validation Loss: 991.5253479972719\n",
      "Epoch 38/100, Train Loss: 33.4310426513612\n",
      "Epoch 38/100, Validation Loss: 978.2603643508185\n",
      "Epoch 39/100, Train Loss: 33.163540438438\n",
      "Epoch 39/100, Validation Loss: 966.2093350849455\n",
      "Epoch 40/100, Train Loss: 32.95807507505867\n",
      "Epoch 40/100, Validation Loss: 951.9576745411706\n",
      "Epoch 41/100, Train Loss: 32.690385149164634\n",
      "Epoch 41/100, Validation Loss: 940.381589859251\n",
      "Epoch 42/100, Train Loss: 32.482836249842926\n",
      "Epoch 42/100, Validation Loss: 925.0123639787946\n",
      "Epoch 43/100, Train Loss: 32.254059066441044\n",
      "Epoch 43/100, Validation Loss: 915.1925571986607\n",
      "Epoch 44/100, Train Loss: 32.015294453437335\n",
      "Epoch 44/100, Validation Loss: 899.8438216920883\n",
      "Epoch 45/100, Train Loss: 31.75832055484412\n",
      "Epoch 45/100, Validation Loss: 883.4059051029266\n",
      "Epoch 46/100, Train Loss: 31.58394835554621\n",
      "Epoch 46/100, Validation Loss: 877.9218149336558\n",
      "Epoch 47/100, Train Loss: 31.31130518627442\n",
      "Epoch 47/100, Validation Loss: 864.7878698924231\n",
      "Epoch 48/100, Train Loss: 31.115550990984087\n",
      "Epoch 48/100, Validation Loss: 853.2168346586682\n",
      "Epoch 49/100, Train Loss: 30.94076023112744\n",
      "Epoch 49/100, Validation Loss: 845.2705688476562\n",
      "Epoch 50/100, Train Loss: 30.867247323774514\n",
      "Epoch 50/100, Validation Loss: 846.5528603205605\n",
      "Epoch 51/100, Train Loss: 30.913084493343526\n",
      "Epoch 51/100, Validation Loss: 844.1973353794643\n",
      "Epoch 52/100, Train Loss: 30.74446708664388\n",
      "Epoch 52/100, Validation Loss: 832.7776188926091\n",
      "Epoch 53/100, Train Loss: 30.560425318272166\n",
      "Epoch 53/100, Validation Loss: 824.2074168371776\n",
      "Epoch 54/100, Train Loss: 30.430432765705582\n",
      "Epoch 54/100, Validation Loss: 818.7399127294146\n",
      "Epoch 55/100, Train Loss: 30.29938002268651\n",
      "Epoch 55/100, Validation Loss: 812.7323395259797\n",
      "Epoch 56/100, Train Loss: 30.19446188639979\n",
      "Epoch 56/100, Validation Loss: 806.7862655397446\n",
      "Epoch 57/100, Train Loss: 30.024998963452767\n",
      "Epoch 57/100, Validation Loss: 805.1867346385169\n",
      "Epoch 58/100, Train Loss: 29.995706149449717\n",
      "Epoch 58/100, Validation Loss: 793.4781852601067\n",
      "Epoch 59/100, Train Loss: 29.852234497729285\n",
      "Epoch 59/100, Validation Loss: 785.1682894267733\n",
      "Epoch 60/100, Train Loss: 29.54680437740809\n",
      "Epoch 60/100, Validation Loss: 774.8946126302084\n",
      "Epoch 61/100, Train Loss: 29.4749304062279\n",
      "Epoch 61/100, Validation Loss: 773.8426407102554\n",
      "Epoch 62/100, Train Loss: 29.60354939033438\n",
      "Epoch 62/100, Validation Loss: 773.9164990621899\n",
      "Epoch 63/100, Train Loss: 29.354834687772996\n",
      "Epoch 63/100, Validation Loss: 757.5211423843626\n",
      "Epoch 64/100, Train Loss: 29.08171501726931\n",
      "Epoch 64/100, Validation Loss: 751.4952973865327\n",
      "Epoch 65/100, Train Loss: 29.01833342056003\n",
      "Epoch 65/100, Validation Loss: 749.8365294441344\n",
      "Epoch 66/100, Train Loss: 28.99218513403299\n",
      "Epoch 66/100, Validation Loss: 752.0003865559896\n",
      "Epoch 67/100, Train Loss: 29.002459693581947\n",
      "Epoch 67/100, Validation Loss: 740.6427098834325\n",
      "Epoch 68/100, Train Loss: 28.771131613073294\n",
      "Epoch 68/100, Validation Loss: 732.3790874178447\n",
      "Epoch 69/100, Train Loss: 28.63338438293928\n",
      "Epoch 69/100, Validation Loss: 725.1601281544519\n",
      "Epoch 70/100, Train Loss: 28.53885954796055\n",
      "Epoch 70/100, Validation Loss: 737.4394008091518\n",
      "Epoch 71/100, Train Loss: 28.764951585483058\n",
      "Epoch 71/100, Validation Loss: 736.0124162946429\n",
      "Epoch 72/100, Train Loss: 28.64588359055659\n",
      "Epoch 72/100, Validation Loss: 727.942394438244\n",
      "Epoch 73/100, Train Loss: 28.45249519584951\n",
      "Epoch 73/100, Validation Loss: 716.6354767330109\n",
      "Epoch 74/100, Train Loss: 28.350587804442817\n",
      "Epoch 74/100, Validation Loss: 713.6438850523933\n",
      "Epoch 75/100, Train Loss: 28.21930480074925\n",
      "Epoch 75/100, Validation Loss: 707.0402144174727\n",
      "Epoch 76/100, Train Loss: 28.09992133051742\n",
      "Epoch 76/100, Validation Loss: 705.0744754851811\n",
      "Epoch 77/100, Train Loss: 27.973312113311053\n",
      "Epoch 77/100, Validation Loss: 695.7593781001984\n",
      "Epoch 78/100, Train Loss: 27.900632176044162\n",
      "Epoch 78/100, Validation Loss: 693.0499587286087\n",
      "Epoch 79/100, Train Loss: 27.736413443469345\n",
      "Epoch 79/100, Validation Loss: 687.3838307214162\n",
      "Epoch 80/100, Train Loss: 27.647062352913675\n",
      "Epoch 80/100, Validation Loss: 682.8103647383433\n",
      "Epoch 81/100, Train Loss: 27.719620244400577\n",
      "Epoch 81/100, Validation Loss: 690.1220150902158\n",
      "Epoch 82/100, Train Loss: 27.708583059477345\n",
      "Epoch 82/100, Validation Loss: 687.5405535016741\n",
      "Epoch 83/100, Train Loss: 27.609858153444954\n",
      "Epoch 83/100, Validation Loss: 682.6438346741692\n",
      "Epoch 84/100, Train Loss: 27.508029939379583\n",
      "Epoch 84/100, Validation Loss: 673.8429681687128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100, Train Loss: 27.35354046342597\n",
      "Epoch 85/100, Validation Loss: 666.3734092106895\n",
      "Epoch 86/100, Train Loss: 27.196259887772115\n",
      "Epoch 86/100, Validation Loss: 660.0455254448784\n",
      "Epoch 87/100, Train Loss: 27.09318664397169\n",
      "Epoch 87/100, Validation Loss: 652.4657709030878\n",
      "Epoch 88/100, Train Loss: 26.992786642884425\n",
      "Epoch 88/100, Validation Loss: 648.299329969618\n",
      "Epoch 89/100, Train Loss: 26.960703217858605\n",
      "Epoch 89/100, Validation Loss: 658.8888239784847\n",
      "Epoch 90/100, Train Loss: 27.07242172146449\n",
      "Epoch 90/100, Validation Loss: 653.4811401367188\n",
      "Epoch 91/100, Train Loss: 27.019642639236153\n",
      "Epoch 91/100, Validation Loss: 653.1562974717882\n",
      "Epoch 92/100, Train Loss: 26.93435386380343\n",
      "Epoch 92/100, Validation Loss: 647.5029916914683\n",
      "Epoch 93/100, Train Loss: 26.855486063901843\n",
      "Epoch 93/100, Validation Loss: 648.2989317878844\n",
      "Epoch 94/100, Train Loss: 26.793037354632446\n",
      "Epoch 94/100, Validation Loss: 647.0781957232763\n",
      "Epoch 95/100, Train Loss: 27.098966904034985\n",
      "Epoch 95/100, Validation Loss: 664.8108801463294\n",
      "Epoch 96/100, Train Loss: 27.11481244360813\n",
      "Epoch 96/100, Validation Loss: 658.403552827381\n",
      "Epoch 97/100, Train Loss: 26.894347688702027\n",
      "Epoch 97/100, Validation Loss: 642.9359063042534\n",
      "Epoch 98/100, Train Loss: 26.802663501661744\n",
      "Epoch 98/100, Validation Loss: 658.9166937934028\n",
      "Epoch 99/100, Train Loss: 27.034889261610836\n",
      "Epoch 99/100, Validation Loss: 652.6282474578373\n",
      "Epoch 100/100, Train Loss: 27.26860236086724\n",
      "Epoch 100/100, Validation Loss: 655.7725684756324\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN2D, self).__init__()\n",
    "        self.conv_layer = nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(3, 3))\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add a channel dimension (channels=1) for the 2D CNN\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 1D CNN network for extracting state representative vectors from frequency representative vectors\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv_layer = nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=3)\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add a channel dimension (channels=1) for the 1D CNN\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# LSTM network for state vector prediction\n",
    "class LSTMNetwork(nn.Module):\n",
    "    \n",
    "#     def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "#         super(LSTMNetwork, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.unsqueeze(1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "#     def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "#         super(LSTMNetwork, self).__init__()\n",
    "#         self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.unsqueeze(1)\n",
    "#         h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(device)\n",
    "#         out, _ = self.rnn(x, h0)\n",
    "#         out = self.output_layer(out[:, -1, :])\n",
    "#         return out\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMNetwork, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(device)\n",
    "        c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.output_layer(out[:, -1, :])\n",
    "        return out\n",
    "hori = 50\n",
    "cr = 512\n",
    "mode = 'lstm'\n",
    "input_size = 40800 #2400 #4960 #10080 #20320 #40800 \n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = cr*hori \n",
    "num_filters = 32\n",
    "kernel_size = 3\n",
    "pool_kernel_size = 2\n",
    "pool_stride = 2\n",
    "\n",
    "\n",
    "# Load data from CSV\n",
    "csv_file_path = f'dataset/Uma_{cr}.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "input_data = df.values \n",
    "target_data = input_data.copy()  \n",
    "input_windows = []\n",
    "target_windows = []\n",
    "window_size = 12\n",
    "\n",
    "for i in range(len(input_data) - window_size - hori):\n",
    "    input_windows.append(input_data[i:i+window_size])\n",
    "    target_windows.append(target_data[i+window_size:i+window_size+hori])\n",
    "    \n",
    "print(np.shape(input_windows))\n",
    "print(np.shape(target_windows))\n",
    "\n",
    "input_windows = torch.tensor(input_windows, dtype=torch.float)\n",
    "target_windows = torch.tensor(target_windows, dtype=torch.float)\n",
    "\n",
    "dataset = TensorDataset(input_windows, target_windows)\n",
    "\n",
    "train_ratio = 0.7\n",
    "valid_ratio = 0.2\n",
    "test_ratio = 1 - train_ratio - valid_ratio\n",
    "train_size = int(train_ratio * len(input_windows))\n",
    "valid_size = int((train_ratio + valid_ratio) * len(input_windows)) - int(train_ratio * len(input_windows))\n",
    "test_size = len(input_windows) - int((train_ratio + valid_ratio) * len(input_windows))\n",
    "\n",
    "print(train_size, valid_size, test_size)\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cnn_2d_net = CNN2D().to(device)\n",
    "lstm_net = LSTMNetwork(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "print(cnn_2d_net)\n",
    "print(lstm_net)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(list(cnn_2d_net.parameters()) + list(lstm_net.parameters()), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "avg_loss_lstm = np.zeros(100)\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_2d_net.train()\n",
    "    lstm_net.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    cnt = 0\n",
    "    for inp, targets in train_loader:\n",
    "        if cnt < 198:\n",
    "#             print(cnt)\n",
    "            cnt += 1\n",
    "            inp = inp.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output_2d = cnn_2d_net(inp)\n",
    "            output_lstm = lstm_net(output_2d)\n",
    "            output_lstm = torch.reshape(output_lstm, (32,hori,-1))\n",
    "#             print(output_lstm.size())\n",
    "            loss = (criterion(output_lstm, targets))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss_lstm[epoch] = np.sqrt(total_loss / len(train_loader))\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss_lstm[epoch]}\")\n",
    "\n",
    "    # Validation loop\n",
    "    cnn_2d_net.eval()\n",
    "    lstm_net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0.0\n",
    "        cnt = 0\n",
    "        for inp, targets in valid_loader:\n",
    "            if cnt < 50:\n",
    "                cnt += 1\n",
    "                inp = inp.to(device)\n",
    "                targets = targets.to(device)\n",
    "#                 print(cnt)\n",
    "                output_2d = cnn_2d_net(inp)\n",
    "                output_lstm = lstm_net(output_2d)\n",
    "                output_lstm = torch.reshape(output_lstm, (32,hori,-1))\n",
    "                val_loss = (criterion(output_lstm, targets))\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240caca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "1\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "targets torch.Size([32, 50, 512])\n",
      "prediction torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "torch.Size([32, 50, 512])\n",
      "Root Mean Squared Error (RMSE): 26.746604561805725\n"
     ]
    }
   ],
   "source": [
    "# Assuming you already have the test_dataset from the previous code\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Test loop\n",
    "cnn_2d_net.eval()\n",
    "lstm_net.eval()\n",
    "\n",
    "mse_criterion = nn.MSELoss()\n",
    "total_mse = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    cnt = 0\n",
    "    for inp, targets in test_loader:\n",
    "        if cnt < 30:\n",
    "            cnt += 1\n",
    "            inp = inp.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Pass data sequentially through the networks\n",
    "            output_2d = cnn_2d_net(inp)\n",
    "            output_lstm = lstm_net(output_2d)\n",
    "            output_lstm = torch.reshape(output_lstm, (32,hori,-1))\n",
    "#             print(cnt)\n",
    "            mse_loss = torch.sqrt(mse_criterion(output_lstm, targets))\n",
    "            total_mse += mse_loss.item()\n",
    "            print(\"targets\", targets.size())\n",
    "            print(\"prediction\", output_lstm.size())\n",
    "            \n",
    "            target = targets.to('cpu')\n",
    "            predict = output_lstm.to('cpu')\n",
    "            print(target.size())\n",
    "#             torch.save(predict[:,:,:],\"predict_rnn_5120.pt\")\n",
    "#             torch.save(target[:,:,:],\"target_rnn_5120.pt\")\n",
    "            print(predict.size())\n",
    "#             print(cnt)\n",
    "            if cnt == 1:\n",
    "                print(cnt)\n",
    "                torch.save(predict[:,:,:],f\"predict_{mode}_{cr}_{hori}.pt\")\n",
    "                torch.save(target[:,:,:],f\"target_{mode}_{cr}_{hori}.pt\")\n",
    "\n",
    "avg_mse = total_mse / len(test_loader)\n",
    "print(f\"Root Mean Squared Error (RMSE): {avg_mse}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae944547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826cd83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df30b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46e664d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([979, 512])\n",
      "torch.Size([979, 512])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "csv_file_path = 'output/Uma_512/test/predict_512_9.csv'\n",
    "predict = pd.read_csv(csv_file_path)\n",
    "# \n",
    "csv_file_path = 'output/Uma_512/test/target_512_9.csv'\n",
    "target = pd.read_csv(csv_file_path)\n",
    "\n",
    "predict = torch.tensor(predict.values)\n",
    "target = torch.tensor(target.values)\n",
    "# target = targets.to('cpu')\n",
    "# predict = output_lstm.to('cpu')\n",
    "print(target.size())\n",
    "print(predict.size())\n",
    "\n",
    "torch.save(predict[:,:],\"predict_stem_512_9.pt\")\n",
    "torch.save(target[:,:],\"target_stem_512_9.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "136e1d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cr = 128\n",
    "hori = 3\n",
    "H_test_stem = torch.load(f'F:\\\\matrix\\\\csip\\\\{cr}\\\\H_test_{cr}_stem')\n",
    "H_test_stem = H_test_stem[0:96,:,:]\n",
    "H_test_lstm = torch.load(f'F:\\\\matrix\\\\csip\\\\{cr}\\\\H_test_{cr}_lstm')\n",
    "H_test_rnn = torch.load(f'F:\\\\matrix\\\\csip\\\\{cr}\\\\H_test_{cr}_rnn')\n",
    "# H_test_fnn = torch.load(f'F:\\\\matrix\\\\{cr}\\\\H_test_{cr}_fnn_{hori}')\n",
    "\n",
    "H_hat_stem = torch.load(f'F:\\\\matrix\\\\csip\\\\{cr}\\\\H_hat_{cr}_stem')\n",
    "H_hat_stem = H_hat_stem[0:96,:,:]\n",
    "H_hat_lstm = torch.load(f'F:\\\\matrix\\\\csip\\\\{cr}\\\\H_hat_{cr}_lstm')\n",
    "H_hat_rnn = torch.load(f'F:\\\\matrix\\\\csip\\\\{cr}\\\\H_hat_{cr}_rnn')\n",
    "# H_hat_fnn = torch.load(f'F:\\\\matrix\\\\{cr}\\\\H_hat_{cr}_fnn_{hori}')\n",
    "\n",
    "\n",
    "# print(H_test_stem.size())\n",
    "# print(H_test_lstm.size())\n",
    "# print(H_hat_stem.size())\n",
    "# print(H_hat_lstm.size())\n",
    "\n",
    "def waterfilling(H,P):\n",
    "    U, g, VT = svd(H)\n",
    "#     print(np.shape(VT))\n",
    "    alpha_low = 0 # Initial low\n",
    "    alpha_high = (P + np.sum(1/g**2)) # Initial high\n",
    "\n",
    "    stop_threshold = 1e-7 # Stop threshold\n",
    "\n",
    "    # Iterate while low/high bounds are further than stop_threshold\n",
    "    while(np.abs(alpha_low - alpha_high) > stop_threshold):\n",
    "        alpha = (alpha_low + alpha_high) / 2 # Test value in the middle of low/high\n",
    "\n",
    "        # Solve the power allocation\n",
    "        p = 1/alpha - 1/g**2 \n",
    "        p[p < 0] = 0 # Consider only positive power allocation\n",
    "\n",
    "        # Test sum-power constraints\n",
    "        if (np.sum(p) > P): # Exceeds power limit => lower the upper bound\n",
    "            alpha_low = alpha\n",
    "        else: # Less than power limit => increase the lower bound\n",
    "            alpha_high = alpha\n",
    "    return p\n",
    "\n",
    "def sumrate(ebn,num, H_stem, Ht_stem, H_lstm, Ht_lstm, H_rnn, Ht_rnn, H_fnn, Ht_fnn):\n",
    "\n",
    "\n",
    "    snr = 10**(ebn/10)\n",
    "    sr = []\n",
    "    sr_stem = []\n",
    "    sr_lstm = []\n",
    "    sr_rnn  = []\n",
    "    sr_fnn  = []\n",
    "    \n",
    "    power = np.zeros(32)\n",
    "    p = 0\n",
    "    capacity = 0\n",
    "    \n",
    "    power_stem = np.zeros(32)\n",
    "    p_stem = 0\n",
    "    capacity_stem = 0\n",
    "    \n",
    "    power_lstm = np.zeros(32)\n",
    "    p_lstm = 0\n",
    "    capacity_lstm = 0\n",
    "    \n",
    "    power_rnn = np.zeros(32)\n",
    "    p_rnn = 0\n",
    "    capacity_rnn = 0\n",
    "    \n",
    "    power_fnn = np.zeros(32)\n",
    "    p_fnn = 0\n",
    "    capacity_fnn = 0\n",
    "    \n",
    "    H_stem = H_stem/np.linalg.norm(H_stem)\n",
    "    Ht_stem = Ht_stem/np.linalg.norm(Ht_stem)\n",
    "    \n",
    "    H_lstm = H_lstm/np.linalg.norm(H_lstm)\n",
    "    Ht_lstm = Ht_lstm/np.linalg.norm(Ht_lstm)\n",
    "    \n",
    "    H_rnn = H_rnn/np.linalg.norm(H_rnn)\n",
    "    Ht_rnn = Ht_rnn/np.linalg.norm(Ht_rnn)\n",
    "    \n",
    "#     H_fnn = H_fnn/np.linalg.norm(H_fnn)\n",
    "#     Ht_fnn = Ht_fnn/np.linalg.norm(Ht_fnn)\n",
    "    \n",
    "#     H = H_test_stem[num]/np.linalg.norm(H_test_stem[num])\n",
    "#     Ht = H_hat_stem[num]/np.linalg.norm(H_hat_stem[num])\n",
    "#     print(np.shape(H))\n",
    "#     p_stem = 0\n",
    "#     p_stem = waterfilling(H_hat_stem[num],snr)\n",
    "#     print(p_stem)\n",
    "    \n",
    "    for k in range(32):\n",
    "        power[k] = np.abs(np.reshape(H_lstm[:,k],[1,32]) @ np.reshape(H_lstm[:,k].conj().T, [32,1]))**2 \n",
    "        power[k] = power[k]/(np.abs(np.reshape(H_lstm[:,k],[1,32]) @ np.reshape(H_lstm[:,k].conj().T, [32,1]))**2)\n",
    "#         power[k] = power[k]/(np.linalg.norm(np.reshape(H_lstm[:,k].conj().T, [32,1])/(np.abs(np.reshape(H_lstm[:,k],[1,32]) @ np.reshape(H_lstm[:,k].conj().T, [32,1]))**2)))\n",
    "    \n",
    "    p = np.abs(power)**2\n",
    "    for i in range(32):\n",
    "        capacity += np.log2(1 + p[k]*snr/32)/32;\n",
    "    sr.append(capacity)\n",
    "    \n",
    "    for k in range(32):\n",
    "        power_stem[k] = np.abs(np.reshape(H_stem[:,k],[1,32]) @ np.reshape(Ht_stem[:,k].conj().T, [32,1]))**2 \n",
    "        power_stem[k] = power_stem[k]/(np.abs(np.reshape(Ht_stem[:,k],[1,32]) @ np.reshape(Ht_stem[:,k].conj().T, [32,1]))**2)\n",
    "#         power_stem[k] = power_stem[k]/(np.linalg.norm(np.reshape(Ht_stem[:,k].conj().T, [32,1])/(np.abs(np.reshape(Ht_stem[:,k],[1,32]) @ np.reshape(Ht_stem[:,k].conj().T, [32,1]))**2)))\n",
    "    \n",
    "    p_stem = np.abs(power_stem)**2\n",
    "    for i in range(32):\n",
    "        capacity_stem += np.log2(1 + p_stem[k]*snr/32)/32;\n",
    "    sr_stem.append(capacity_stem)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in range(32):\n",
    "        power_lstm[k] = np.abs(np.reshape(H_lstm[:,k],[1,32]) @ np.reshape(Ht_lstm[:,k].conj().T, [32,1]))**2 \n",
    "        power_lstm[k] = power_lstm[k]/(np.abs(np.reshape(Ht_lstm[:,k],[1,32]) @ np.reshape(Ht_lstm[:,k].conj().T, [32,1]))**2)\n",
    "#         power_lstm[k] = power_lstm[k]/(np.linalg.norm(np.reshape(Ht_lstm[:,k].conj().T, [32,1])/(np.abs(np.reshape(Ht_lstm[:,k],[1,32]) @ np.reshape(Ht_lstm[:,k].conj().T, [32,1]))**2)))\n",
    "    \n",
    "    p_lstm = np.abs(power_lstm)**2\n",
    "    for i in range(32):\n",
    "        capacity_lstm += np.log2(1 + p_lstm[k]*snr/32)/32;\n",
    "    sr_lstm.append(capacity_lstm)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in range(32):\n",
    "        power_rnn[k] = np.abs(np.reshape(H_rnn[:,k],[1,32]) @ np.reshape(Ht_rnn[:,k].conj().T, [32,1]))**2 \n",
    "        power_rnn[k] = power_rnn[k]/(np.abs(np.reshape(Ht_rnn[:,k],[1,32]) @ np.reshape(Ht_rnn[:,k].conj().T, [32,1]))**2)\n",
    "#         power_rnn[k] = power_rnn[k]/(np.linalg.norm(np.reshape(Ht_rnn[:,k].conj().T, [32,1])/(np.abs(np.reshape(Ht_rnn[:,k],[1,32]) @ np.reshape(Ht_rnn[:,k].conj().T, [32,1]))**2)))\n",
    "    \n",
    "    p_rnn = np.abs(power_rnn)**2\n",
    "    for i in range(32):\n",
    "        capacity_rnn += np.log2(1 + p_rnn[k]*snr/32)/32;\n",
    "    sr_rnn.append(capacity_rnn)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for k in range(32):\n",
    "#         power_fnn[k] = np.abs(np.reshape(H_fnn[:,k],[1,32]) @ np.reshape(Ht_fnn[:,k].conj().T, [32,1]))**2 \n",
    "#         power_fnn[k] = power_fnn[k]/(np.abs(np.reshape(Ht_fnn[:,k],[1,32]) @ np.reshape(Ht_fnn[:,k].conj().T, [32,1]))**2)\n",
    "# #         power_fnn[k] = power_fnn[k]/(np.linalg.norm(np.reshape(Ht_fnn[:,k].conj().T, [32,1])/(np.abs(np.reshape(Ht_fnn[:,k],[1,32]) @ np.reshape(Ht_fnn[:,k].conj().T, [32,1]))**2)))\n",
    "    \n",
    "#     p_fnn = np.abs(power_fnn)**2\n",
    "#     for i in range(32):\n",
    "#         capacity_fnn += np.log2(1 + p_fnn[k]*snr/32)/32;\n",
    "#     sr_fnn.append(capacity_fnn)\n",
    "    \n",
    "    \n",
    "    return np.asarray(sr), np.asarray(sr_stem), np.asarray(sr_lstm), np.asarray(sr_rnn), np.asarray(sr_fnn)\n",
    "\n",
    "    \n",
    "many = 96\n",
    "ebn = list(range(0,31,5))\n",
    "snr = [10**(x/10) for x in ebn]\n",
    "sr_128 = [[] for i in range(many)]\n",
    "sr_stem_128 = [[] for i in range(many)]\n",
    "sr_lstm_128 = [[] for i in range(many)]\n",
    "sr_rnn_128 = [[] for i in range(many)]\n",
    "sr_fnn = [[] for i in range(many)]\n",
    "\n",
    "for i in ebn:\n",
    "    print(i)\n",
    "    for j in range(many):\n",
    "#         print(j)\n",
    "        sr0, sr1, sr2, sr3, sr4 = sumrate(i,j, H_test_stem[j], H_hat_stem[j], H_test_lstm[j], H_hat_lstm[j], H_test_rnn[j], H_hat_rnn[j], H_test_rnn[j], H_hat_rnn[j])  \n",
    "        sr_128[j].append(sr0)\n",
    "        sr_stem_128[j].append(sr1)\n",
    "        sr_lstm_128[j].append(sr2)\n",
    "        sr_rnn_128[j].append(sr3)\n",
    "        sr_fnn[j].append(sr4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3fb717f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt \n",
    "\n",
    "# plt.plot(ebn,np.mean(sr, axis=0), label='perfect, Nt=32, Nc=32', marker = 'X', ms = 7, c = 'm')\n",
    "plt.plot(ebn,np.mean(sr_stem_512, axis=0), label=r'STEM, $N_{t}=32$, $N_{c}=32$, $\\gamma = 1/4$', marker = 'o', ms = 7, c = 'b', linewidth=2)\n",
    "plt.plot(ebn,np.mean(sr_rnn_512, axis=0), label=r'LSTM, $N_{t}=32$, $N_{c}=32$, $\\gamma = 1/4$', marker = 's', ms = 6, c = 'y', linewidth=2)\n",
    "plt.plot(ebn,np.mean(sr_lstm_512, axis=0), label=r'RNN, $N_{t}=32$, $N_{c}=32$, $\\gamma = 1/4$', marker = '*', ms = 8, c = 'r', linewidth=2)\n",
    "\n",
    "plt.plot(ebn,np.mean(sr_stem_128, axis=0), label=r'STEM, $N_{t}=32$, $N_{c}=32$, $\\gamma = 1/8$', marker = 'o', ms = 7, c = 'g', linewidth=2)\n",
    "plt.plot(ebn,np.mean(sr_rnn_256, axis=0), label=r'LSTM, $N_{t}=32$, $N_{c}=32$, $\\gamma = 1/8$', marker = 's', ms = 6, c = 'c', linewidth=2)\n",
    "plt.plot(ebn,np.mean(sr_lstm_256, axis=0), label=r'RNN, $N_{t}=32$, $N_{c}=32$, $\\gamma = 1/8$', marker = '*', ms = 8, c = 'm', linewidth=2)\n",
    "\n",
    "plt.plot(ebn,np.mean(sr_stem_256, axis=0), label=r'STEM, $N_{t}=32$, $N_{c}=32$, $\\gamma = 1/16$', marker = 'o', ms = 7, c = 'k', linewidth=2)\n",
    "plt.plot(ebn,np.mean(sr_rnn_128, axis=0), label=r'LSTM, $N_{t}=32$, $N_{c}=32$, $\\gamma = 1/16$', marker = 's', ms = 6, c = 'tab:brown', linewidth=2)\n",
    "plt.plot(ebn,np.mean(sr_lstm_128, axis=0), label=r'RNN, $N_{t}=32$, $N_{c}=32$, $\\gamma = 1/16$', marker = '*', ms = 8, c = 'tab:orange', linewidth=2)\n",
    "\n",
    "\n",
    "# plt.plot(ebn,np.mean(sr_fnn , axis=0), label='fnn, Nt=32, Nc=32', marker = '^', ms = 6, c = 'g')\n",
    "plt.xlabel(r'$E_{b}/N_{0} (dB)$', fontsize = 12)\n",
    "plt.ylabel(\"Spectral Efficiency (bps/Hz)\", fontsize = 12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a3620ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_stem_512 = [-3.61166927774, -3.08451961459, -2.83590100139, -2.51328908166]\n",
    "mse_stem_256 = [-3.52781830193, -2.96968595073, -2.73166795075, -2.46595246334]\n",
    "mse_stem_128 = [-3.19103927299, -2.93013932428, -2.61020223448, -2.37514778743]\n",
    "\n",
    "mse_lstm_128 = [-0.92867873502, -0.90506769294, -0.86569178499, -0.8324122678]\n",
    "mse_lstm_256 = [-0.80334218188, -0.79374516037, -0.80191702337, -0.7683017809]\n",
    "mse_lstm_512 = [-0.76180752636, -0.77114846960, -0.78916990878, -0.7550146195]\n",
    "\n",
    "mse_rnn_128 = [-0.98761271365, -0.95354782156, -0.89697948274, -0.8788428984]\n",
    "mse_rnn_256 = [-0.89534566440, -0.93959614139, -0.82591949072, -0.8361662438]\n",
    "mse_rnn_512 = [-0.88676419129, -0.79372990646, -0.80417621209, -0.7994353978]\n",
    "\n",
    "barWidth = 0.1\n",
    "fig = plt.subplots(figsize =(12, 8))\n",
    "horis = [3, 5, 7, 9]\n",
    "\n",
    "br1 = np.arange(len(horis))\n",
    "br2 = [x + barWidth for x in br1]\n",
    "br3 = [x + barWidth for x in br2]\n",
    "br4 = [x + barWidth for x in br3]\n",
    "br5 = [x + barWidth for x in br4]\n",
    "br6 = [x + barWidth for x in br5]\n",
    "br7 = [x + barWidth for x in br6]\n",
    "br8 = [x + barWidth for x in br7]\n",
    "br9 = [x + barWidth for x in br8]\n",
    "\n",
    "plt.bar(br1, mse_stem_128, color ='y', width = barWidth,\n",
    "        edgecolor ='grey', label ='STEM-128')\n",
    "plt.bar(br2, mse_stem_256, color ='tab:orange', width = barWidth,\n",
    "        edgecolor ='grey', label ='STEM-256')\n",
    "plt.bar(br3, mse_stem_512, color ='r', width = barWidth,\n",
    "        edgecolor ='grey', label ='STEM-512')\n",
    "\n",
    "plt.bar(br4, mse_rnn_128, color ='mediumseagreen', width = barWidth,\n",
    "        edgecolor ='grey', label ='LSTM-128')\n",
    "plt.bar(br5, mse_rnn_256, color ='tab:green', width = barWidth,\n",
    "        edgecolor ='grey', label ='LSTM-256')\n",
    "plt.bar(br6, mse_rnn_512, color ='g', width = barWidth,\n",
    "        edgecolor ='grey', label ='LSTM-512')\n",
    "\n",
    "\n",
    "plt.bar(br7, mse_lstm_128, color ='xkcd:sky blue', width = barWidth,\n",
    "        edgecolor ='grey', label ='RNN-128')\n",
    "plt.bar(br8, mse_lstm_256, color ='tab:blue', width = barWidth,\n",
    "        edgecolor ='grey', label ='RNN-256')\n",
    "plt.bar(br9, mse_lstm_512, color ='b', width = barWidth,\n",
    "        edgecolor ='grey', label ='RNN-512')\n",
    " \n",
    "# Adding Xticks\n",
    "plt.xlabel('Horizon', fontsize = 12)\n",
    "plt.ylabel('NMSE (dB) after Channel Reconstruction', fontsize = 12)\n",
    "plt.xticks([r + 4*barWidth for r in range(len(horis))],\n",
    "        ['3', '5', '7', '9'])\n",
    "# plt.ylim(-4, 0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "44390fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_stem_512 = [1.5832, 2.6189, 4.1774, 5.4101]\n",
    "rmse_stem_256 = [1.9206, 3.0388, 5.2129, 5.6265]\n",
    "rmse_stem_128 = [2.5506, 3.3334, 4.4479, 6.0265]\n",
    "\n",
    "rmse_lstm_128 = [27.193055741, 26.6125009050, 26.7770670050, 27.7291775343]\n",
    "rmse_lstm_256 = [24.729510545, 24.9839926950, 24.7412368050, 25.2967482805]\n",
    "rmse_lstm_512 = [24.440444529, 23.7080699205, 23.9851315617, 24.3868720531]\n",
    "\n",
    "rmse_rnn_128 = [25.2789484262, 24.2054104208, 24.9343266487, 24.8131954073]\n",
    "rmse_rnn_256 = [23.0762034058, 22.9603829382, 23.6214694383, 24.7122474310]\n",
    "rmse_rnn_512 = [22.4404445290, 22.7080699205, 22.9851315617, 24.3868720531]\n",
    "\n",
    "barWidth = 0.1\n",
    "fig = plt.subplots(figsize =(12, 8))\n",
    "horis = [3, 5, 7, 9]\n",
    "\n",
    "br1 = np.arange(len(horis))\n",
    "br2 = [x + barWidth for x in br1]\n",
    "br3 = [x + barWidth for x in br2]\n",
    "br4 = [x + barWidth for x in br3]\n",
    "br5 = [x + barWidth for x in br4]\n",
    "br6 = [x + barWidth for x in br5]\n",
    "br7 = [x + barWidth for x in br6]\n",
    "br8 = [x + barWidth for x in br7]\n",
    "br9 = [x + barWidth for x in br8]\n",
    "\n",
    "plt.bar(br1, rmse_stem_128, color ='y', width = barWidth,\n",
    "        edgecolor ='grey', label ='STEM-128')\n",
    "plt.bar(br2, rmse_stem_256, color ='tab:orange', width = barWidth,\n",
    "        edgecolor ='grey', label ='STEM-256')\n",
    "plt.bar(br3, rmse_stem_512, color ='r', width = barWidth,\n",
    "        edgecolor ='grey', label ='STEM-512')\n",
    "\n",
    "plt.bar(br4, rmse_rnn_128, color ='mediumseagreen', width = barWidth,\n",
    "        edgecolor ='grey', label ='LSTM-128')\n",
    "plt.bar(br5, rmse_rnn_256, color ='tab:green', width = barWidth,\n",
    "        edgecolor ='grey', label ='LSTM-256')\n",
    "plt.bar(br6, rmse_rnn_512, color ='g', width = barWidth,\n",
    "        edgecolor ='grey', label ='LSTM-512')\n",
    "\n",
    "\n",
    "plt.bar(br7, rmse_lstm_128, color ='xkcd:sky blue', width = barWidth,\n",
    "        edgecolor ='grey', label ='RNN-128')\n",
    "plt.bar(br8, rmse_lstm_256, color ='tab:blue', width = barWidth,\n",
    "        edgecolor ='grey', label ='RNN-256')\n",
    "plt.bar(br9, rmse_lstm_512, color ='b', width = barWidth,\n",
    "        edgecolor ='grey', label ='RNN-512')\n",
    " \n",
    "# Adding Xticks\n",
    "plt.xlabel('Horizon', fontsize = 12)\n",
    "plt.ylabel('RMSE after Compressed Channel Prediction', fontsize = 12)\n",
    "plt.xticks([r + 4*barWidth for r in range(len(horis))],\n",
    "        ['3', '5', '7', '9'])\n",
    "# plt.ylim(-4, 0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c0ed265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_stem = [0.2698, 0.0473, 0.0117, 0.0086, 0.0059, 0.0053, 0.0047, 0.0043, 0.0040, 0.0034, 0.0033, 0.0031, 0.0030, 0.0029, 0.0027, 0.0026, 0.0026, 0.0025, 0.0025, 0.0024, 0.0024, 0.0023, 0.0023, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0021, 0.0021, 0.0021, 0.0021, 0.0021, 0.0021, 0.0021, 0.0021, 0.0021, 0.0021]\n",
    "train_rmse_stem = [0.2983, 0.1806, 0.1396, 0.1140, 0.1230, 0.0893, 0.0834, 0.0800, 0.0763, 0.0759, 0.0712, 0.0690, 0.0672, 0.0654, 0.0651, 0.0634, 0.0634, 0.0621, 0.0612, 0.0614, 0.0605, 0.0600, 0.0597, 0.0593, 0.0591, 0.0590, 0.0586, 0.0585, 0.0584, 0.0581, 0.0580, 0.0579, 0.0579, 0.0577, 0.0576, 0.0575, 0.0575, 0.0575, 0.0574, 0.0573, 0.0573]\n",
    "# plt.plot(train_rmse_stem)\n",
    "# plt.plot(avg_loss[0:40])\n",
    "# plt.xlabel('epochs',fontweight ='bold', fontsize = 12)\n",
    "# plt.ylabel('RMSE loss during training',fontweight ='bold', fontsize = 12)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "lns1 = ax.plot(train_rmse_stem, '-', label = 'STEM', color ='y', linewidth=2)\n",
    "ax2 = ax.twinx()\n",
    "lns2 = ax2.plot(avg_loss_rnn[0:40], '-', label = 'LSTM', color ='mediumseagreen', linewidth=2)\n",
    "lns3 = ax2.plot(avg_loss_lstm[0:40], '-', label = 'RNN', color ='xkcd:sky blue', linewidth=2)\n",
    "\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('epochs', fontsize = 12)\n",
    "ax.set_ylabel('RMSE loss during training for STEM', fontsize = 12)\n",
    "ax2.set_ylabel('RMSE loss during training for LSTM/RNN', fontsize = 12)\n",
    "lns = lns1+lns2+lns3\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs, loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c7d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
